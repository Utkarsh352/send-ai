
# OpenRouter configuration (for cloud models)
OPENROUTER_API_KEY=key

# Local model configuration (optional)
# Set USE_LOCAL_MODEL=true to use local models instead of OpenRouter
USE_LOCAL_MODEL=false

# For Ollama (default)
LOCAL_MODEL_URL=http://localhost:11434/v1
LOCAL_MODEL_NAME=llama3.2:latest
LOCAL_MODEL_API_KEY=ollama

# For LM Studio
# LOCAL_MODEL_URL=http://localhost:1234/v1
# LOCAL_MODEL_NAME=local-model
# LOCAL_MODEL_API_KEY=lm-studio

# For LocalAI
# LOCAL_MODEL_URL=http://localhost:8080/v1
# LOCAL_MODEL_NAME=gpt-3.5-turbo
# LOCAL_MODEL_API_KEY=localai

# For any OpenAI-compatible local server
# LOCAL_MODEL_URL=http://localhost:8000/v1
# LOCAL_MODEL_NAME=your-model-name
# LOCAL_MODEL_API_KEY=your-api-key
